{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4df619b2-759b-4142-aa57-a961718c7e16",
   "metadata": {},
   "source": [
    "## Text Classification - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb06ceeb-d03f-4b74-b137-5cda660b7d39",
   "metadata": {},
   "source": [
    "Supervised learning - We know what are the target labels are.  \n",
    "Unsupervised learning - We do not know what the target labels are.  \n",
    "Semi Supervised learning - Combination of both labeled and unlabeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365946f3-c8d2-47f8-a814-48745a53ebf1",
   "metadata": {},
   "source": [
    "### Classification blueprint. \n",
    "Steps:  \n",
    "    1. Preparation of the train and test data.  \n",
    "    2. Text normalization  \n",
    "    3. Feature extraction  \n",
    "    4. Model training  \n",
    "    5. Model prediction  \n",
    "    6. Model evaluation  \n",
    "    7. Model deployment  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed0ff80-6901-4249-b66f-2e5a9251adc4",
   "metadata": {},
   "source": [
    "#### 1. Preparation of train and test data. \n",
    "If data is of 10 lines then use 8 lines to train the model and 2 lines to test the train model.  \n",
    "So the data is to be split.  \n",
    "sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef788b78-ff8f-4a14-a4db-fb9dc83025aa",
   "metadata": {},
   "source": [
    "#### 2. Text Normalization. \n",
    "1. Stemming - It divides words into suffix, prefix and stem and gives the stem \n",
    "\n",
    "| Form | Suffix | Stem          |\n",
    "| ------------- | ------------- | ------------- |\n",
    "| Studies  | -es  | studi       |\n",
    "| Studying  | -ing  | study     |\n",
    "\n",
    "This is not a best way to do because the stem word is not coming as an exact english word. <br/>\n",
    "2. Lemmatization - Best than stemming, it has intelligence and gives the proper english word. <br/>\n",
    "\n",
    "| Form | Lemma |\n",
    "| ------------- | ------------- |\n",
    "| Studies  | Study  |\n",
    "| Studying  | Study  |\n",
    "\n",
    "<br/>\n",
    "3. Removal of stop - words like articles, etc. <br/>\n",
    "4. Special characters - Like punctuations, pronouns, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51d69b6a-d6f2-4a32-82ab-5ddca9293f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\biswa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\biswa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\biswa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "feb0ff18-d659-418c-84b7-447a67430133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello hello\n",
      "how how\n",
      "are are\n",
      "you you\n",
      "doing do\n",
      "? ?\n",
      "I i\n",
      "am am\n",
      "done done\n",
      "with with\n",
      "the the\n",
      "example exampl\n",
      "for for\n",
      "this thi\n",
      "class class\n"
     ]
    }
   ],
   "source": [
    "# Stemming and Lemmatization\n",
    "sentence = \"Hello how are you doing? I am done with the example for this class\"\n",
    "ps = PorterStemmer()\n",
    "_words = word_tokenize(sentence)\n",
    "for word in _words:\n",
    "    print(word,ps.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab589743-9d8c-4f3f-912e-4fcb8063ca2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Hello\n",
      "how how\n",
      "are are\n",
      "you you\n",
      "doing doing\n",
      "? ?\n",
      "I I\n",
      "am am\n",
      "done done\n",
      "with with\n",
      "the the\n",
      "example example\n",
      "for for\n",
      "this this\n",
      "class class\n"
     ]
    }
   ],
   "source": [
    "lemma = WordNetLemmatizer()\n",
    "_words = word_tokenize(sentence)\n",
    "for word in _words:\n",
    "    print(word,lemma.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6eb6327b-4617-42e5-b991-2c7e426b0fa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['If',\n",
       " 'Easter',\n",
       " 'Bunny',\n",
       " 'Tooth',\n",
       " 'fairy',\n",
       " 'babies',\n",
       " 'would',\n",
       " 'take',\n",
       " 'teeth',\n",
       " 'leave',\n",
       " 'chocolate',\n",
       " '?']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removal of Stopwords\n",
    "stopword = set(stopwords.words('english'))\n",
    "sentence = \"If the Easter Bunny and the Tooth fairy had babies would they take your teeth and leave chocolate for you?\"\n",
    "_words = word_tokenize(sentence)\n",
    "filtered_sentence = [w for w in _words if w not in stopword]\n",
    "filtered_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ae536e-e17a-4061-8259-61795394ebef",
   "metadata": {},
   "source": [
    "#### 3. Feature Extraction\n",
    "- Features are unique, measurable attributes are properties for each observation or data point in a data set.  \n",
    "- Features are usually numeric - we need categorical data for a classifier.  \n",
    "- What s vectorization? Converting the textual data into numerical values with respect to the frequency or occurrence of the word.  \n",
    "2 types of Vectorization. \n",
    "1. Bag of words aka CountVectorizer\n",
    "2. TF - IDF Model --> Term Frequency - Inverse document frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1622d1a6-746b-44e0-9843-001998bf0e70",
   "metadata": {},
   "source": [
    "##### Count Vectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fb510b1-5747-48ac-a08b-c7f53d2f8896",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4aec727-4667-49f2-8b4f-31923f02f287",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2bd54ae-5e5a-4187-bcf8-9ddcf7ce802e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 8,\n",
       " 'is': 3,\n",
       " 'the': 6,\n",
       " 'first': 2,\n",
       " 'document': 1,\n",
       " 'second': 5,\n",
       " 'and': 0,\n",
       " 'third': 7,\n",
       " 'one': 4}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = CountVectorizer() # Model created\n",
    "vec.fit(corpus) # Model's vocabulary created\n",
    "vec.vocabulary_ # Getting Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d93baaf-4093-463d-8043-5015d56aceee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 1, 0, 1],\n",
       "       [0, 2, 0, 1, 0, 1, 1, 0, 1],\n",
       "       [1, 0, 0, 1, 1, 0, 1, 1, 1],\n",
       "       [0, 1, 1, 1, 0, 0, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.transform(corpus).toarray() # Vectorizing the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "044f3192-89a6-4c4e-b837-524cf8358fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "counts = vec.fit_transform(corpus)\n",
    "print(vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bdcf7dfa-f2cb-4662-9829-38786b238a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(counts.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d87092-a76d-4ba8-88d8-19346b420a79",
   "metadata": {},
   "source": [
    "##### Tf - Idf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bac4f1ca-be3c-4b89-b5fc-2b92250ab711",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a772d93c-1247-4f00-ad94-008fd4658080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "tfIdf = TfidfVectorizer()\n",
    "frequency = tfIdf.fit_transform(corpus)\n",
    "#tfidf weighted document-term matrix is created from fit_transform\n",
    "print(tfIdf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "599affec-8a69-4585-8432-d4a4566eafc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[0.        , 0.46979139, 0.58028582, 0.38408524, 0.        ,\n",
      "        0.        , 0.38408524, 0.        , 0.38408524],\n",
      "       [0.        , 0.6876236 , 0.        , 0.28108867, 0.        ,\n",
      "        0.53864762, 0.28108867, 0.        , 0.28108867],\n",
      "       [0.51184851, 0.        , 0.        , 0.26710379, 0.51184851,\n",
      "        0.        , 0.26710379, 0.51184851, 0.26710379],\n",
      "       [0.        , 0.46979139, 0.58028582, 0.38408524, 0.        ,\n",
      "        0.        , 0.38408524, 0.        , 0.38408524]])\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent = 1)\n",
    "pp.pprint(frequency.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d74f34-a67b-4e07-83a3-a7b0950242c2",
   "metadata": {},
   "source": [
    "#### 6. Model Evaluation\n",
    "- True Positive - TP\n",
    "- True Negative - TN\n",
    "- False Positive - FP\n",
    "- False Negative - FN\n",
    "<br/>\n",
    "- Accuracy : Proportion of correct predictions of the model  \n",
    "    Accuracy = (TP + TN)/(TP + TN + FN + FP)\n",
    "- Precision : Number of predictions made that are accurately correct based on positive class  \n",
    "    Precision = TP / (TP + FP)\n",
    "- Recall : Number of instances that of the positive class that were correctly predicted  \n",
    "    Recall = TP / (TP + FN)\n",
    "- F-1 Score: Is another accuracy measure which is the harmonic mean of precision and recall.  \n",
    "    F1 Score = (2 x Precision x Recall)/(Precision + Recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5727da-5c94-477d-8de5-bd3b71bbb272",
   "metadata": {},
   "source": [
    "# Text Classification 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9189995c-2450-4c27-a0de-0919b83a444b",
   "metadata": {},
   "source": [
    "### Fit and Transform. \n",
    "- The fit() function when applied on training datasets, learns model parameters.\n",
    "- The transform() function is applied on both train and test dataset.  \n",
    "- This function of fit and transform on training dataset could be done at once using fit_transform() <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d026413b-3349-4e40-b180-61515aee9b36",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier\n",
    "- Is the most important text classification algorithm and also extremely fast when compared to other classification.  \n",
    "- Naive (Simple) classification is based on Bayes theorem of probability to predict the class of unknown data.  \n",
    "- Bayes theorem equation is as follows:  \n",
    "                  P(c|x) = $ \\frac {P(x|c)*P(c)}{P(x)}$ <br/> \n",
    "                  where c is class, x is attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "019e409d-46db-4c63-8716-6c9a12d88146",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "#MultinomialNB is one of the naive_bayes algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fcd78c74-bb81-4467-8f74-58b1d477a2e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Nah I dont think he goes to usf, he lives arou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>1.0</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Will  b going to esplanade fr home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Pity,  was in mood for that. So...any other su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5572</th>\n",
       "      <td>0.0</td>\n",
       "      <td>The guy did some bitching but I acted like id ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5573</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5574 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                            message\n",
       "0       0.0  Go until jurong point, crazy.. Available only ...\n",
       "1       0.0                      Ok lar... Joking wif u oni...\n",
       "2       1.0  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3       0.0  U dun say so early hor... U c already then say...\n",
       "4       0.0  Nah I dont think he goes to usf, he lives arou...\n",
       "...     ...                                                ...\n",
       "5569    1.0  This is the 2nd time we have tried 2 contact u...\n",
       "5570    0.0                 Will  b going to esplanade fr home\n",
       "5571    0.0  Pity,  was in mood for that. So...any other su...\n",
       "5572    0.0  The guy did some bitching but I acted like id ...\n",
       "5573    0.0                         Rofl. Its true to its name\n",
       "\n",
       "[5574 rows x 2 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('SMSSpamCollection.csv', sep='|', names = ['label','message'])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b9fe52bc-a366-45f6-9a86-b92ea1f9aae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\biswa\\AppData\\Local\\Temp/ipykernel_56008/358439218.py:5: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data['message'] = data.message.str.replace('[^\\w\\s]', '')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>go until jurong point crazy available only in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>u dun say so early hor u c already then say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>nah i dont think he go to usf he life around h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>1.0</td>\n",
       "      <td>this is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>0.0</td>\n",
       "      <td>will b going to esplanade fr home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>0.0</td>\n",
       "      <td>pity wa in mood for that soany other suggestion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5572</th>\n",
       "      <td>0.0</td>\n",
       "      <td>the guy did some bitching but i acted like id ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5573</th>\n",
       "      <td>0.0</td>\n",
       "      <td>rofl it true to it name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5574 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                            message\n",
       "0       0.0  go until jurong point crazy available only in ...\n",
       "1       0.0                            ok lar joking wif u oni\n",
       "2       1.0  free entry in 2 a wkly comp to win fa cup fina...\n",
       "3       0.0        u dun say so early hor u c already then say\n",
       "4       0.0  nah i dont think he go to usf he life around h...\n",
       "...     ...                                                ...\n",
       "5569    1.0  this is the 2nd time we have tried 2 contact u...\n",
       "5570    0.0                  will b going to esplanade fr home\n",
       "5571    0.0    pity wa in mood for that soany other suggestion\n",
       "5572    0.0  the guy did some bitching but i acted like id ...\n",
       "5573    0.0                            rofl it true to it name\n",
       "\n",
       "[5574 rows x 2 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text Normalization\n",
    "lemma = WordNetLemmatizer()\n",
    "# data['label'] = data.label.map({'ham': 0, 'spam': 1})  \n",
    "data['message'] = data.message.map(lambda x: x.lower())  \n",
    "data['message'] = data.message.str.replace('[^\\w\\s]', '')  \n",
    "data['message'] = data['message'].apply(word_tokenize)\n",
    "data['message'] = data['message'].apply(lambda x: [lemma.lemmatize(y) for y in x])  \n",
    "data['message'] = data['message'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "cf3a98e0-40cb-4c01-bdf3-8a7fe50b62da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CountVec = CountVectorizer()\n",
    "counts = CountVec.fit_transform(data['message'])\n",
    "# CountVec.vocabulary_\n",
    "counts.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d7c22c21-0b05-479d-ab15-a3bdc9008c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    counts, data[\"label\"], test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "46e37853-8a6f-4685-944f-0a5ed3b7b76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "aab6cdaa-f33b-4050-89eb-83a6cf18b037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., ..., 0., 1., 1.])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = model.predict(X_test)\n",
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8cd40f22-0ee7-4689-b869-48d73b6a28a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9748878923766816\n"
     ]
    }
   ],
   "source": [
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac37e2c-623c-4215-8a4f-3988ad52de0f",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier\n",
    "- Decision tree is a type of supervised learning algorithm that is mostly used in classification problems.\n",
    "- Let’s say we have a sample of 30 students with three variables Gender (Boy/ Girl), Grade( IX/ X) and Height (5 to 6 ft). 15 out of these 30 play soccer in leisure time.\n",
    "- With the information we need to split the students who play soccer in their leisure time based on highly significant input variable among all three.\n",
    "<br/>\n",
    "- One method is to classify based on outcome of the criterion which leads to homogenous value.\n",
    "- For example, if the data is split w.r.t gender will it lead to homogenous nodes or is it w.r.t height or it is w.r.t grades.\n",
    "<br/>\n",
    "\n",
    "#### Disadvantage of this model is\n",
    "Since it is supervised learning when it is trained very well, and if test data is given then accuracy of the model decreases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2fb825a4-498e-4602-9f04-e81f500aa0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "df4ff3d3-8899-415b-9ee0-cfe5402b535b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    go until jurong point crazy available only in ...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['message'].iloc[[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f46055bb-b146-4f21-8880-ce553cede84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CountVec = CountVectorizer()\n",
    "counts = CountVec.fit_transform(data['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "472274c2-f345-4926-810a-5c12d79d4f02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_df = CountVec.transform(data['message'].iloc[[0]])\n",
    "counts_df.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "691d47e3-b8b7-47e9-a71a-a07a2269aa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test , y_train, y_test = train_test_split(\n",
    "    counts, data['label'], test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2281249a-0640-4444-96b8-b9df47f57e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5f448424-57f6-441e-a508-2220adf4d489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = model.predict(counts_df)\n",
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "fa49508a-3020-49d3-94ee-1b6d4736354d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9524663677130045\n"
     ]
    }
   ],
   "source": [
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5092f7d-74b3-4914-83fd-4f0b7a2a76f1",
   "metadata": {},
   "source": [
    "### Random Forests\n",
    "- Ensemble of trees classification is called Random Forest Classifier.\n",
    "- Random forest classifier creates a set of decision trees from randomly selected subset of training set.\n",
    "- It then aggregates the votes from different decision trees to decide the final class of the test object.\n",
    "- Random forest in a way is much more accurate than decision tree classifier.\n",
    "<br/>\n",
    "\n",
    "#### Disadvantage\n",
    "- It takes more time than the Decision Tree Classifier and Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3666d305-259a-4e55-9e7d-37d50df167ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
